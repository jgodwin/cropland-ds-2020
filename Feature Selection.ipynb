{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "We have 10 images with 5 bands each. We know that some of the images are relatively close to one another temporally, and therefore expect that some of these images are redundant and can be removed, or combined in some way to reduce the feature space.  This script attempts to determine which (if any) of these features can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "import common, tqdm, numpy as np\n",
    "import pandas as pd\n",
    "import preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the images and normalize all of the bands by converting their values to Z-values, which as we determined earlier was appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:43<00:00, 10.39s/it]\n"
     ]
    }
   ],
   "source": [
    "images = preprocess.loadZValues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with PCA to determine how much redundancy there is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: DeprecationWarning: This function is deprecated. Please call randint(0, 19014446 + 1) instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components='mle', random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import random\n",
    "h,w,_ = images[0][1].shape\n",
    "selected_idx = random.random_integers(0,h*w,25000)\n",
    "\n",
    "subset = [im[1][:,:,band].flatten()[selected_idx].reshape(-1,1) for im in images for band in range(im[1].shape[2]) ]\n",
    "\n",
    "features = np.concatenate(subset,axis=1)\n",
    "print(features.shape)\n",
    "pca = decomposition.PCA(n_components='mle')\n",
    "pca.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component 0 : explained variance: 0.57, cum explained variance 0.57\n",
      "component 1 : explained variance: 0.10, cum explained variance 0.67\n",
      "component 2 : explained variance: 0.08, cum explained variance 0.75\n",
      "component 3 : explained variance: 0.04, cum explained variance 0.79\n",
      "component 4 : explained variance: 0.03, cum explained variance 0.83\n",
      "component 5 : explained variance: 0.03, cum explained variance 0.85\n",
      "component 6 : explained variance: 0.02, cum explained variance 0.88\n",
      "component 7 : explained variance: 0.02, cum explained variance 0.89\n",
      "component 8 : explained variance: 0.02, cum explained variance 0.91\n",
      "component 9 : explained variance: 0.01, cum explained variance 0.92\n",
      "component 10 : explained variance: 0.01, cum explained variance 0.93\n",
      "component 11 : explained variance: 0.01, cum explained variance 0.94\n",
      "component 12 : explained variance: 0.01, cum explained variance 0.95\n",
      "component 13 : explained variance: 0.01, cum explained variance 0.96\n",
      "component 14 : explained variance: 0.01, cum explained variance 0.97\n",
      "component 15 : explained variance: 0.01, cum explained variance 0.97\n",
      "component 16 : explained variance: 0.01, cum explained variance 0.98\n",
      "component 17 : explained variance: 0.00, cum explained variance 0.98\n",
      "component 18 : explained variance: 0.00, cum explained variance 0.99\n",
      "component 19 : explained variance: 0.00, cum explained variance 0.99\n",
      "component 20 : explained variance: 0.00, cum explained variance 0.99\n",
      "component 21 : explained variance: 0.00, cum explained variance 0.99\n",
      "component 22 : explained variance: 0.00, cum explained variance 0.99\n",
      "component 23 : explained variance: 0.00, cum explained variance 0.99\n",
      "component 24 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 25 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 26 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 27 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 28 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 29 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 30 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 31 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 32 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 33 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 34 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 35 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 36 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 37 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 38 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 39 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 40 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 41 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 42 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 43 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 44 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 45 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 46 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 47 : explained variance: 0.00, cum explained variance 1.00\n",
      "component 48 : explained variance: 0.00, cum explained variance 1.00\n"
     ]
    }
   ],
   "source": [
    "ratio = pca.explained_variance_ratio_\n",
    "cumRatio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "for k, ratios in enumerate(zip(ratio,cumRatio)):\n",
    "    r1, c1 = ratios\n",
    "    print('component {} : explained variance: {:.2f}, cum explained variance {:.2f}'.format(k,r1,c1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA transformation has automatically estimated that we need 49 components to reconstruct the original data, but by looking at the explained variance ratios we can see that the first principal component explains about 56% of the variance in the dataset. Subsequent components (out to about the 15th component) account for about 97% of the variance in the original data. PCA is clearly showing that there is a significant amount of redundancy in this data, which maybe is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd518045978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn\n",
    "seaborn.heatmap(abs(pca.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a heatmap of the PCA principal components where each row is a single component, and each column represents the contribution from each individual feature.  For PCA to be maximally useful, we'd like to see that the main components have a clear meaning in feature space, e.g. the first principal component is band1 of image 1 and band 3 of image 4.  Unfortunately in this case, it looks like most of the components are mixed combinations of multiple bands from multiple images simultaneously.  I would note though that it looks like the 2nd or third principal component may be just a combination of the 5th band from each image, which is interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component:  0 [0.12384352 0.13302112 0.12318528 0.13241464 0.05433704 0.15032274\n",
      " 0.15091398 0.14694573 0.14932235 0.03955434 0.158548   0.16174862\n",
      " 0.15697551 0.16020432 0.02508592 0.1625632  0.16481063 0.15825588\n",
      " 0.16072737 0.02303109 0.16486393 0.16648425 0.1603151  0.1594478\n",
      " 0.03534589 0.16467056 0.16587658 0.16012263 0.15701465 0.03058382\n",
      " 0.15600777 0.15901567 0.15432794 0.15056066 0.06813902 0.1573029\n",
      " 0.16051185 0.15660863 0.15864536 0.06894961 0.15430543 0.15635459\n",
      " 0.15433633 0.1554553  0.08881591 0.16060226 0.16112192 0.15776198\n",
      " 0.15685485 0.0950228 ]\n",
      "component:  1 [-0.10937796 -0.11579666 -0.12133425 -0.16169336 -0.07043201 -0.06645647\n",
      " -0.08019239 -0.06242517 -0.14150953 -0.22381851  0.08685331  0.05526679\n",
      "  0.08984254 -0.03652627 -0.36352298  0.07449858  0.04805976  0.08643958\n",
      " -0.04094175 -0.3764759   0.06030219  0.04004849  0.07531764 -0.01998697\n",
      " -0.3520056   0.03183958  0.01359012  0.05118063 -0.06377921 -0.33557585\n",
      "  0.05274951  0.04766878  0.06451853 -0.01492358 -0.25947243  0.09932867\n",
      "  0.09098247  0.1092103   0.01863158 -0.27540532  0.10062736  0.09995095\n",
      "  0.11278853  0.03012963 -0.22093149  0.04308651  0.05031389  0.03114188\n",
      " -0.00995072 -0.07924556]\n",
      "component:  2 [-0.24936248 -0.22685859 -0.27300066 -0.13961352  0.34093368 -0.20581183\n",
      " -0.21061184 -0.22120535 -0.17083219  0.2647663   0.04859383  0.04647099\n",
      "  0.07923407  0.06208651 -0.04526122  0.02757777  0.02470425  0.04078398\n",
      "  0.03249811  0.00203305 -0.0887307  -0.09576645 -0.08468851 -0.0957104\n",
      "  0.05475937 -0.10702412 -0.11178785 -0.11523557 -0.10756619  0.17771088\n",
      " -0.01315976 -0.0055193  -0.012212    0.01076093  0.14310662  0.11749679\n",
      "  0.12371295  0.12047821  0.13711165  0.12224943  0.14813754  0.16128233\n",
      "  0.1587362   0.18350123  0.17766266  0.0450666   0.07059876  0.03107313\n",
      "  0.10596005  0.21838802]\n",
      "component:  3 [ 0.00919092 -0.01252967  0.00703764 -0.03627863 -0.17007361 -0.01765987\n",
      " -0.01824173 -0.00792238 -0.01878178 -0.14055172 -0.22660984 -0.23030695\n",
      " -0.251698   -0.2242906   0.05856515 -0.2523805  -0.25012824 -0.26965192\n",
      " -0.2332344   0.08240468 -0.11103302 -0.10998522 -0.13277006 -0.09188942\n",
      "  0.12961598  0.07057958  0.0753288   0.07503127  0.09233554 -0.07544079\n",
      "  0.2196866   0.21934825  0.23633954  0.23831636 -0.00747918  0.13472216\n",
      "  0.13579193  0.14267936  0.14725983 -0.02894955  0.11219418  0.11460004\n",
      "  0.11269356  0.11932993  0.0250487   0.08511458  0.08049357  0.07972169\n",
      "  0.07460689  0.0415279 ]\n",
      "component:  4 [ 0.2623837   0.25172698  0.24079454  0.22226763  0.09345578  0.12260413\n",
      "  0.12041444  0.10958976  0.09824704  0.05703694  0.03231671  0.01915601\n",
      "  0.00287213 -0.02512247 -0.05626233 -0.0091892  -0.02956168 -0.04485747\n",
      " -0.08284959 -0.0560078  -0.17173943 -0.19193977 -0.21435587 -0.2486567\n",
      " -0.04400563 -0.15195352 -0.17128466 -0.19499718 -0.21882191  0.01640933\n",
      " -0.15271938 -0.16552758 -0.1659459  -0.21745391 -0.22249979  0.10179844\n",
      "  0.08792362  0.08378095  0.04333396 -0.12999874  0.11567671  0.1112675\n",
      "  0.11004301  0.07692406 -0.06484052  0.1514572   0.15885921  0.13540652\n",
      "  0.14424099  0.10064269]\n",
      "component:  5 [ 0.04579019  0.0476717   0.04803864  0.03476176  0.10872347  0.10091762\n",
      "  0.09636751  0.08901584  0.07774612  0.05490523  0.14910272  0.15419835\n",
      "  0.14305921  0.13632546 -0.02087053  0.04162319  0.03773249  0.03218547\n",
      "  0.00316037 -0.06312773 -0.14881577 -0.15520652 -0.17281188 -0.18878935\n",
      "  0.03034377 -0.13066922 -0.12626769 -0.13887548 -0.14604427  0.03919112\n",
      "  0.25094825  0.25094777  0.25259107  0.27438003  0.15417536  0.10390751\n",
      "  0.0977148   0.11243749  0.08432585 -0.08301882 -0.05050568 -0.06200159\n",
      " -0.04758136 -0.09680082 -0.20444076 -0.24872157 -0.24884196 -0.27115119\n",
      " -0.24878651 -0.08661858]\n",
      "component:  6 [-0.20891471 -0.23618849 -0.15823013 -0.35194674 -0.3710967   0.18867067\n",
      "  0.17004299  0.20273888  0.09675086 -0.31187177  0.07978068  0.10663535\n",
      "  0.06386642  0.11979506  0.13246703  0.04469444  0.0713623   0.034273\n",
      "  0.09063227  0.13869856 -0.0650325  -0.04285881 -0.09220789 -0.02373884\n",
      "  0.2183454  -0.12513584 -0.11425038 -0.14973608 -0.09829552  0.18435901\n",
      " -0.05186114 -0.05363747 -0.0583668  -0.04115373  0.06345923 -0.00084515\n",
      " -0.00179804  0.02703469  0.01878066 -0.06071656  0.0706633   0.05593554\n",
      "  0.0694548   0.04560953 -0.07935528  0.12672178  0.09750059  0.17755103\n",
      "  0.04843721 -0.23338515]\n",
      "component:  7 [-0.06804317 -0.05119128 -0.02029952 -0.03360049 -0.07947209  0.1256142\n",
      "  0.1213944   0.11960229  0.11070514 -0.02247845  0.08405307  0.06047792\n",
      "  0.07285957  0.00178453 -0.21248212 -0.05931764 -0.08042234 -0.06120042\n",
      " -0.14160767 -0.23952857 -0.03130194 -0.047674   -0.02863254 -0.08423443\n",
      " -0.22905959 -0.08085112 -0.07890313 -0.07471228 -0.090501   -0.11465158\n",
      "  0.10328083  0.11316729  0.06990878  0.1488184   0.33839405 -0.1695624\n",
      " -0.14720976 -0.18725069 -0.14196452  0.20689158 -0.15925981 -0.12842692\n",
      " -0.15665552 -0.09580554  0.17050593  0.13466507  0.18198575  0.1648998\n",
      "  0.25929916  0.32842457]\n",
      "component:  8 [ 0.01535234  0.01352314  0.05888936  0.01443704 -0.12238248  0.09168879\n",
      "  0.09484604  0.12062629  0.09481514 -0.07940428 -0.04129071 -0.05417827\n",
      " -0.03157935 -0.08792534 -0.2030219  -0.02893863 -0.03866699 -0.0202027\n",
      " -0.07132882 -0.20544262  0.02369018  0.03010089  0.03576858  0.03587052\n",
      " -0.15686627 -0.05599269 -0.04248931 -0.05389518 -0.03412783 -0.030577\n",
      " -0.09745004 -0.07802331 -0.11502391 -0.08037955  0.11480111  0.04838736\n",
      "  0.07789776  0.03305929  0.15983702  0.42864668  0.1370834   0.15823182\n",
      "  0.11343491  0.22885337  0.43700787 -0.16938756 -0.190629   -0.15498823\n",
      " -0.22507048 -0.2889192 ]\n",
      "component:  9 [ 0.20803209  0.16473745  0.14960966  0.10661528 -0.05582969 -0.14042252\n",
      " -0.16442719 -0.18584171 -0.20842867 -0.09193622 -0.0610366  -0.08609071\n",
      " -0.110734   -0.14136605 -0.07780232  0.13050827  0.12033664  0.09937386\n",
      "  0.10692366  0.00591245  0.09353516  0.08073087  0.04906147  0.06237939\n",
      "  0.02718799 -0.13196579 -0.1550089  -0.1962659  -0.18124306  0.11050159\n",
      "  0.14119248  0.11656647  0.06912763  0.13408332  0.35553432 -0.14206521\n",
      " -0.1687687  -0.17557108 -0.21117905 -0.16097292  0.17265663  0.1474719\n",
      "  0.13524252  0.11119424 -0.00276075  0.11315361  0.06419539  0.0965865\n",
      " -0.01267501 -0.26190284]\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    print('component: ', k, pca.components_[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PCA is not really illuminating which features are most important, another way to take a stab at figuring out which features are important is to fit a simple model to the data and use attributes of the resultant model to figure out which features are most important. Tree based models are usually a good choice for this approach, I prefer Random Forests for this task since they're simple and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = common.loadNumpy('labels')\n",
    "y = labels.squeeze().flatten()[selected_idx].reshape(-1,1)\n",
    "\n",
    "xtr,xte,ytr,yte = train_test_split(features,y,train_size=0.8)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtr,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(yte,ypr):\n",
    "    p,r,f,s  =precision_recall_fscore_support(yte,ypr,\n",
    "                                              average=None)\n",
    "    df = pd.DataFrame()\n",
    "    df.index.name = 'Class'\n",
    "    df['Precision'] = p\n",
    "    df['Recall'] = r\n",
    "    df['F1'] = f\n",
    "    df['Support'] = s\n",
    "    print(df)\n",
    "\n",
    "    print(confusion_matrix(yte,ypr))\n",
    "    \n",
    "def score(classifier, xtr,ytr,xte,yte):\n",
    "    print('train score', classifier.score(xtr,ytr))\n",
    "    print('test score', classifier.score(xte,yte))\n",
    "\n",
    "    print('>>> Confusion Matrix Train <<<')\n",
    "    conf_matrix(ytr, classifier.predict(xtr))\n",
    "    print('>>> Confusion Matrix Test <<<')\n",
    "    conf_matrix(yte,classifier.predict(xte))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9871\n",
      "test score 0.7256\n",
      ">>> Confusion Matrix Train <<<\n",
      "    Precision    Recall        F1  Support\n",
      "0    0.983607  0.992556  0.988061     1209\n",
      "1    0.983521  0.998282  0.990847     8729\n",
      "2    0.985813  0.987414  0.986613     4926\n",
      "3    0.996801  0.992989  0.994891     1569\n",
      "4    0.996032  0.917174  0.954978      821\n",
      "5    0.989346  0.996933  0.993125      652\n",
      "6    1.000000  0.979048  0.989413      525\n",
      "7    0.997732  0.946237  0.971302      465\n",
      "8    0.992268  0.969773  0.980892      397\n",
      "9    0.990991  0.942857  0.966325      350\n",
      "10   1.000000  0.935574  0.966715      357\n",
      "[[1200    2    4    0    0    1    0    1    0    1    0]\n",
      " [   2 8714   11    0    0    0    0    0    2    0    0]\n",
      " [   3   58 4864    1    0    0    0    0    0    0    0]\n",
      " [   2    4    4 1558    0    0    0    0    0    1    0]\n",
      " [   2   55   10    1  753    0    0    0    0    0    0]\n",
      " [   1    1    0    0    0  650    0    0    0    0    0]\n",
      " [   1    3    1    0    0    6  514    0    0    0    0]\n",
      " [   2    6   15    0    1    0    0  440    1    0    0]\n",
      " [   4    5    3    0    0    0    0    0  385    0    0]\n",
      " [   1    0   18    1    0    0    0    0    0  330    0]\n",
      " [   2   12    4    2    2    0    0    0    0    1  334]]\n",
      ">>> Confusion Matrix Test <<<\n",
      "    Precision    Recall        F1  Support\n",
      "0    0.542683  0.570513  0.556250      312\n",
      "1    0.764545  0.908163  0.830189     2156\n",
      "2    0.737016  0.729796  0.733388     1225\n",
      "3    0.835106  0.767726  0.800000      409\n",
      "4    0.187500  0.073892  0.106007      203\n",
      "5    0.727273  0.736196  0.731707      163\n",
      "6    0.663265  0.515873  0.580357      126\n",
      "7    0.434783  0.176991  0.251572      113\n",
      "8    0.367347  0.166667  0.229299      108\n",
      "9    0.725490  0.397849  0.513889       93\n",
      "10   0.272727  0.097826  0.144000       92\n",
      "[[ 178   54   24    6   15   19    5    2    8    0    1]\n",
      " [  22 1958  121    8   20    3    3    4    5    7    5]\n",
      " [  22  255  894   19    8    1    6    7    6    3    4]\n",
      " [   3   44   29  314    3    8    7    0    1    0    0]\n",
      " [  20  105   33    7   15    3    1    4    4    1   10]\n",
      " [  13    9    0   12    1  120    7    1    0    0    0]\n",
      " [   9   17   11    2    1   10   65    4    5    1    1]\n",
      " [  10   24   47    3    3    1    0   20    2    2    1]\n",
      " [  34   30   17    1    2    0    3    1   18    0    2]\n",
      " [   2   27   22    3    0    0    1    1    0   37    0]\n",
      " [  15   38   15    1   12    0    0    2    0    0    9]]\n"
     ]
    }
   ],
   "source": [
    "score(rf,xtr,ytr,xte,yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs well on the training dataset, but not so well on the test dataset.  Overall the performance is decent, but we're seeing clearly in the confusion matrices that we're not predicting well on class 0, 1, and 2 to a lesser extent in the test dataset.  Since we're not using this for our final model we can ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010906553476275173 0.06322775530024002\n"
     ]
    }
   ],
   "source": [
    "imp = rf.feature_importances_\n",
    "print(imp.min(), imp.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances from the random forest classifier are not super helpful either.  They're showing that the biggest range from the most useful to least useful feature is only about a factor of 4, which is not huge and probably means that we would be better off leaving all of the features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last test to run is to pit PCA's recommended 15 variables in a Random Forest classifier and see how well it performs relative to what we just observed above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pca2 = decomposition.PCA(n_components=15)\n",
    "pca2.fit(features)\n",
    "ftransform = pca2.transform(features)\n",
    "print(ftransform.shape)\n",
    "xtr2,xte2, ytr2,yte2= train_test_split(ftransform,y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 15) (20000, 1) (5000, 15) (5000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier()\n",
    "print(xtr2.shape, ytr2.shape, xte2.shape,yte2.shape)\n",
    "rf2.fit(xtr2,ytr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.9858\n",
      "test score 0.7236\n",
      ">>> Confusion Matrix Train <<<\n",
      "    Precision    Recall        F1  Support\n",
      "0    0.986031  0.989283  0.987654     1213\n",
      "1    0.981397  0.998500  0.989875     8665\n",
      "2    0.985063  0.987055  0.986058     4944\n",
      "3    0.992980  0.986058  0.989507     1578\n",
      "4    0.995973  0.914920  0.953728      811\n",
      "5    0.989280  0.986260  0.987768      655\n",
      "6    0.994197  0.979048  0.986564      525\n",
      "7    0.997773  0.951168  0.973913      471\n",
      "8    1.000000  0.971014  0.985294      414\n",
      "9    0.994030  0.922438  0.956897      361\n",
      "10   0.994203  0.944904  0.968927      363\n",
      "[[1200    5    2    2    1    0    1    0    0    0    2]\n",
      " [   2 8652   10    0    0    0    0    1    0    0    0]\n",
      " [   1   60 4880    1    0    1    1    0    0    0    0]\n",
      " [   1   14    6 1556    0    0    1    0    0    0    0]\n",
      " [   4   53    9    3  742    0    0    0    0    0    0]\n",
      " [   3    3    0    3    0  646    0    0    0    0    0]\n",
      " [   1    2    2    0    0    6  514    0    0    0    0]\n",
      " [   1    5   15    0    1    0    0  448    0    1    0]\n",
      " [   3    3    5    0    1    0    0    0  402    0    0]\n",
      " [   0    5   22    1    0    0    0    0    0  333    0]\n",
      " [   1   14    3    1    0    0    0    0    0    1  343]]\n",
      ">>> Confusion Matrix Test <<<\n",
      "    Precision    Recall        F1  Support\n",
      "0    0.513932  0.538961  0.526149      308\n",
      "1    0.767325  0.892793  0.825318     2220\n",
      "2    0.719875  0.762220  0.740443     1207\n",
      "3    0.858757  0.760000  0.806366      400\n",
      "4    0.312500  0.070423  0.114943      213\n",
      "5    0.666667  0.725000  0.694611      160\n",
      "6    0.555556  0.396825  0.462963      126\n",
      "7    0.454545  0.140187  0.214286      107\n",
      "8    0.365854  0.164835  0.227273       91\n",
      "9    0.673913  0.378049  0.484375       82\n",
      "10   0.133333  0.046512  0.068966       86\n",
      "[[ 166   62   27    8    3   19    5    4    8    3    3]\n",
      " [  25 1982  164   14   12    2    2    3   10    2    4]\n",
      " [  25  224  920    8    7    3    3    4    2    7    4]\n",
      " [   9   56   19  304    1    5    4    1    1    0    0]\n",
      " [  19  119   31    9   15    4    4    0    1    1   10]\n",
      " [  11    5    4    4    1  116   18    0    0    1    0]\n",
      " [  11   18   13    3    0   25   50    1    2    1    2]\n",
      " [  18   29   39    1    2    0    0   15    1    0    2]\n",
      " [  20   27   20    1    1    0    4    2   15    0    1]\n",
      " [   4   18   28    0    0    0    0    1    0   31    0]\n",
      " [  15   43   13    2    6    0    0    2    1    0    4]]\n"
     ]
    }
   ],
   "source": [
    "score(rf2,xtr2,ytr2,xte2,yte2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PCA transformed features, with only 15 components (as we identified earlier), the final result here is pretty similar to what we obtained while using all 50 components.  We lost only about .02 accuracy on the test set, while reducing the feature space by almost 3-fold.  That's a pretty good job, and we'll definitely use that going forward, because it should speed-up our compute time significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041818823217813626 0.12021557784115226\n"
     ]
    }
   ],
   "source": [
    "print(rf2.feature_importances_.min(), rf2.feature_importances_.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the range in the values of the feature importances are smaller when using the PCA transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another way of approaching the feature selection problem, where we use the SelectFromModel class from sklearn with the RF classifier that we fit using all 50 features. This class automatically selects 18 features to use, and based on the train/test score, the result is very close to the original score that we obtained using all 50 features. It's also slightly better than what we obtained using PCA, so this really reinforces the selections that I made using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False,  True,  True, False,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False, False, False,\n",
       "       False,  True, False, False, False,  True,  True, False, False,\n",
       "        True,  True,  True, False, False, False, False,  True,  True,\n",
       "       False, False, False,  True, False, False, False, False,  True,\n",
       "       False, False, False, False,  True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "autoSelector = SelectFromModel(rf,prefit=True)\n",
    "autoSelector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 20)\n"
     ]
    }
   ],
   "source": [
    "xtr3 = autoSelector.transform(xtr)\n",
    "xte3 = autoSelector.transform(xte)\n",
    "print(xtr3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.98645\n",
      "test score 0.7248\n",
      ">>> Confusion Matrix Train <<<\n",
      "    Precision    Recall        F1  Support\n",
      "0    0.978013  0.993383  0.985638     1209\n",
      "1    0.982292  0.997709  0.989940     8729\n",
      "2    0.987031  0.988835  0.987932     4926\n",
      "3    0.993606  0.990440  0.992020     1569\n",
      "4    0.998670  0.914738  0.954863      821\n",
      "5    0.992272  0.984663  0.988453      652\n",
      "6    0.992293  0.980952  0.986590      525\n",
      "7    0.997738  0.948387  0.972437      465\n",
      "8    0.997375  0.957179  0.976864      397\n",
      "9    0.996988  0.945714  0.970674      350\n",
      "10   1.000000  0.935574  0.966715      357\n",
      "[[1201    3    0    1    1    1    0    1    0    1    0]\n",
      " [   4 8709   15    1    0    0    0    0    0    0    0]\n",
      " [   2   49 4871    4    0    0    0    0    0    0    0]\n",
      " [   3    9    3 1554    0    0    0    0    0    0    0]\n",
      " [   3   57    7    2  751    1    0    0    0    0    0]\n",
      " [   3    1    1    2    0  642    3    0    0    0    0]\n",
      " [   3    2    3    0    0    2  515    0    0    0    0]\n",
      " [   2   15    7    0    0    0    0  441    0    0    0]\n",
      " [   5    7    4    0    0    0    1    0  380    0    0]\n",
      " [   1    0   17    0    0    1    0    0    0  331    0]\n",
      " [   1   14    7    0    0    0    0    0    1    0  334]]\n",
      ">>> Confusion Matrix Test <<<\n",
      "    Precision    Recall        F1  Support\n",
      "0    0.532110  0.557692  0.544601      312\n",
      "1    0.765379  0.900278  0.827366     2156\n",
      "2    0.722530  0.746122  0.734137     1225\n",
      "3    0.836317  0.799511  0.817500      409\n",
      "4    0.208333  0.073892  0.109091      203\n",
      "5    0.731707  0.736196  0.733945      163\n",
      "6    0.716049  0.460317  0.560386      126\n",
      "7    0.333333  0.132743  0.189873      113\n",
      "8    0.461538  0.222222  0.300000      108\n",
      "9    0.738095  0.333333  0.459259       93\n",
      "10   0.200000  0.054348  0.085470       92\n",
      "[[ 174   57   27    3    9   16    4    9    6    0    7]\n",
      " [  27 1941  131   19   13    1    1    5    9    6    3]\n",
      " [  19  246  914   17    9    1    0    9    5    1    4]\n",
      " [   4   38   29  327    1    5    4    1    0    0    0]\n",
      " [  13  112   47    5   15    3    0    3    1    1    3]\n",
      " [  16   10    2    5    1  120    9    0    0    0    0]\n",
      " [  10   17    9    7    1   17   58    0    5    1    1]\n",
      " [  10   34   41    2    5    1    1   15    2    2    0]\n",
      " [  29   22   23    1    5    0    2    1   24    0    1]\n",
      " [   5   18   30    4    0    0    2    2    0   31    1]\n",
      " [  20   41   12    1   13    0    0    0    0    0    5]]\n"
     ]
    }
   ],
   "source": [
    "rf3 = RandomForestClassifier()\n",
    "rf3.fit(xtr3,ytr)\n",
    "\n",
    "score(rf3,xtr3,ytr,xte3,yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.array([im[0]+'_'+str(band) for im in images for band in range(im[1].shape[2]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20170306_0', '20170306_4', '20170410_0', '20170410_2',\n",
       "       '20170410_3', '20170410_4', '20170601_0', '20170601_3',\n",
       "       '20170601_4', '20170615_4', '20170708_3', '20170708_4',\n",
       "       '20170807_2', '20170807_3', '20170807_4', '20170905_4',\n",
       "       '20170923_0', '20170923_4', '20171015_4', '20171207_4'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[autoSelector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage to using SelectFromModel over the PCA approach is that we're able to directly interrogate the selected features. I'm showing in the cell above the features that were selected automatically through this approach. Interestingly, it seems to prefer the fifth band (4th index) for most images. Also, it has chosen to drop the images acquired in September, entirely from the feature set which is interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
